{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(Score): \n",
    "    T = np.maximum(0, Score)\n",
    "    return T\n",
    "\n",
    "def Softmax(scores):\n",
    "    probs = np.exp(scores)\n",
    "    probs /= np.sum(probs, keepdims=True)\n",
    "    return np.array(probs)\n",
    "\n",
    "def DNLL(Scores , correctClass):\n",
    "    Scores[0][correctClass] = Scores[0][correctClass] - 1\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoding(Labels):\n",
    "    Encoded = []\n",
    "    T = {\"daisy\":0, \"dandelion\":1, \"roses\":2, \"sunflowers\":3, \"tulips\":4}\n",
    "    for i in Labels: \n",
    "        Encoded.append(T[i])\n",
    "    return np.array(Encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    def __init__ (self, HiddenLayers, Neurons): \n",
    "        self.NeuralNetwork = []\n",
    "        self.HiddenLayers = HiddenLayers\n",
    "        self.Neurons = Neurons\n",
    "        self.weights = []\n",
    "        self.OutputLayer = np.zeros(5).reshape(-1,5)\n",
    "        self.InputLayer = np.zeros(3072).reshape(-1,3072)\n",
    "        \n",
    "        self.NeuralNetwork.append(self.InputLayer)\n",
    "        for i in range(self.HiddenLayers): \n",
    "            HiddenLayer = np.zeros(self.Neurons[i]).reshape(-1, Neurons[i])\n",
    "            self.NeuralNetwork.append(HiddenLayer)\n",
    "        self.NeuralNetwork.append(self.OutputLayer)\n",
    "        \n",
    "    def RGL(self , Regularization): \n",
    "        self.RegLoss = 0; \n",
    "        for W in self.weights[0]: \n",
    "            self.RegLoss += 0.5 * Regularization * np.sum(W*W)\n",
    "        return self.RegLoss\n",
    "        \n",
    "    def SetInput(self, IN): \n",
    "        self.NeuralNetwork[0] = IN\n",
    "        \n",
    "    def WeightIntialization(self):\n",
    "        for i in range(len(self.NeuralNetwork)-1): \n",
    "            D = self.NeuralNetwork[i].shape[1]\n",
    "            H = self.NeuralNetwork[i+1].shape[1]\n",
    "            W = np.random.randn(D,H)/np.sqrt(D)\n",
    "            self.weights.append(W)\n",
    "        self.weights = np.array(self.weights)\n",
    "        \n",
    "    def BiasIntialization(self):\n",
    "        self.Bias = []\n",
    "        for i in range(len(self.NeuralNetwork)-1):\n",
    "            B = np.zeros((1,self.NeuralNetwork[i+1].shape[1]))\n",
    "            self.Bias.append(B)\n",
    "        \n",
    "    def WeightGradientIntialization(self):\n",
    "        self.DW = []\n",
    "        for i in self.weights:\n",
    "            temp = np.zeros(i.shape)\n",
    "            self.DW.append(temp)\n",
    "            \n",
    "    def ScoresGradientIntialization(self):\n",
    "        self.DLayers = []        \n",
    "        for i in range(1, len(self.NeuralNetwork)):\n",
    "            temp = np.zeros(self.NeuralNetwork[i].shape)\n",
    "            self.DLayers.append(temp)\n",
    "    \n",
    "    def BiasGradientIntialization(self): \n",
    "        self.DB = []\n",
    "        for i in self.Bias: \n",
    "            temp = np.zeros(i.shape)\n",
    "            self.DB.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTrain: \n",
    "    def __init__(self, NeuralNetwork , Weights , Bias):\n",
    "        self.NeuralNetwork = NeuralNetwork\n",
    "        self.Weights = Weights\n",
    "        self.Bias = Bias\n",
    "    \n",
    "    def UpdateWeights(self , LearningRate , DW): \n",
    "        for i in range(len(self.Weights)): \n",
    "            self.Weights[i]+= -LearningRate * DW[i]\n",
    "        return self.Weights\n",
    "    \n",
    "    def UpdateBias(self , LearningRate , DB): \n",
    "        for i in range(len(self.Bias)): \n",
    "            self.Bias[i] += -LearningRate * DB[i]\n",
    "        return self.Bias\n",
    "    \n",
    "    def UpdateGradients(self,DW, D_Weights , Regularization):\n",
    "        D_Weights = D_Weights[::-1]\n",
    "        for i in range(len(D_Weights)):\n",
    "            DW[i] += D_Weights[i]\n",
    "        for i in range(len(self.Weights)): \n",
    "            DW[i] += Regularization * self.Weights[i]\n",
    "        return DW\n",
    "    \n",
    "    def ForwardPass(self):\n",
    "        for i in range(len(self.NeuralNetwork) - 1):\n",
    "            temp = np.dot(self.NeuralNetwork[i], self.Weights[i])  + self.Bias[i]\n",
    "            if i+1 == len(self.NeuralNetwork) - 1:\n",
    "                self.NeuralNetwork[i+1]= Softmax(temp)\n",
    "            else:\n",
    "                self.NeuralNetwork[i+1]= Relu(temp)\n",
    "    \n",
    "    def BackPropagation(self , CorrectClass):\n",
    "        self.CorrectClass = CorrectClass\n",
    "        D_Scores = []\n",
    "        D_Weights = []\n",
    "        D_Bias = []\n",
    "        count = 0\n",
    "        for i in range(len(self.NeuralNetwork)-1 , 0 , -1): \n",
    "            if i == len(self.NeuralNetwork)-1: #Last Layer\n",
    "                X = DNLL(self.NeuralNetwork[i] , self.CorrectClass)\n",
    "                Y = np.dot(self.NeuralNetwork[i-1].T , X)\n",
    "                Z = np.sum(X, keepdims = True)\n",
    "                D_Scores.append(X)\n",
    "                D_Weights.append(Y)\n",
    "                D_Bias.append(Z)\n",
    "                count+=1\n",
    "            else: \n",
    "                X = np.dot(D_Scores[count-1] , self.Weights[i].T)\n",
    "                X[self.NeuralNetwork[i] <= 0] = 0                     #DRELU\n",
    "                Y = np.dot(self.NeuralNetwork[i-1].T ,X)\n",
    "                Z = np.sum(X, keepdims = True)\n",
    "                D_Scores.append(X)\n",
    "                D_Weights.append(Y)\n",
    "                D_Bias.append(Z)\n",
    "                count+=1\n",
    "        return D_Weights , D_Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroCenter(PicSet):\n",
    "    PicSet -= np.mean(PicSet, axis = 0)\n",
    "    return PicSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageGradient(Gradients , BatchSize): \n",
    "    Gradients[0] /= BatchSize\n",
    "    return Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadpics(datapath):\n",
    "    dimensions = (32 , 32)\n",
    "    testset = list()\n",
    "    trainingset = list()\n",
    "    testlabels = list()\n",
    "    trainglabels = list()\n",
    "    for folder in os.listdir(datapath):\n",
    "        test = list()\n",
    "        labels = list()\n",
    "        for image in os.listdir(os.path.join(datapath, folder)):\n",
    "            sub = os.path.join(datapath,folder)\n",
    "            imagename = os.path.join(sub , image)\n",
    "            i = Image.open(imagename)\n",
    "            i = i.resize(dimensions)\n",
    "            test.append(np.reshape(np.asarray(i) ,(3,dimensions[0],dimensions[1])).transpose(1,2,0))\n",
    "            labels.append(folder)\n",
    "            \n",
    "            \n",
    "        trainingset.extend(test[:len(test)-134])\n",
    "        testset.extend(test[-134:])\n",
    "        \n",
    "        trainglabels.extend(labels[:len(labels)-134])\n",
    "        testlabels.extend(labels[-134:])\n",
    "    \n",
    "    \n",
    "    trainingset = np.array(trainingset, dtype = \"float64\")\n",
    "    testset = np.array(testset , dtype = \"float64\")\n",
    "    traininglabels = np.array(trainglabels)\n",
    "    testlabels = np.array(testlabels)\n",
    "    \n",
    "    \n",
    "    trainingset = ZeroCenter(trainingset)\n",
    "    testset = ZeroCenter(testset)\n",
    "    \n",
    "    return trainingset,testset,traininglabels, testlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(Images , Labels):\n",
    "    arr = [(image , label) for image, label in zip (Images,Labels)]\n",
    "    np.random.shuffle(arr)\n",
    "    X = [i[0] for i in arr]\n",
    "    Y = [i[1] for i in arr]\n",
    "    return X , Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpochLoss(Set, LabelsSet , NN , NNT):\n",
    "    RegLoss = NN.RGL(0.1)\n",
    "    Epoch_Loss = 0\n",
    "    for i in range(Set.shape[0]):\n",
    "        NN.SetInput(Set[i].reshape(-1 , 3072))\n",
    "        NNT = NeuralNetworkTrain(NN.NeuralNetwork, NN.weights, NN.Bias)\n",
    "        NNT.ForwardPass()\n",
    "        Epoch_Loss += -np.log(NN.NeuralNetwork[-1][0][LabelsSet[i]]) + RegLoss\n",
    "    return Epoch_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:\\\\Users\\\\Seif Eldin Hani\\\\Downloads\\\\flower_photos\\\\flower_photos\\\\\"\n",
    "trainingset,testset,traininglabels, testlabels = loadpics(datapath)\n",
    "np.random.seed(60)\n",
    "trainingset , traininglabels = shuffle(trainingset, traininglabels)\n",
    "\n",
    "trainingset = np.asarray(trainingset)\n",
    "testset = np.asarray(testset)\n",
    "traininglabels = np.asarray(traininglabels)\n",
    "testlabels = np.asarray(testlabels)\n",
    "\n",
    "\n",
    "trainingset = np.reshape(trainingset, (trainingset.shape[0], -1))\n",
    "testset = np.reshape(testset, (testset.shape[0], -1))\n",
    "\n",
    "traininglabels = Encoding(traininglabels)\n",
    "testlabels = Encoding(testlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(3, [100,200,100])\n",
    "NN.WeightIntialization()\n",
    "NN.BiasIntialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 /200, Training Loss = 6.438040 , Validation Loss = 6.711789 \n",
      "Epoch 2 /200, Training Loss = 6.259358 , Validation Loss = 6.653106 \n",
      "Epoch 3 /200, Training Loss = 6.133502 , Validation Loss = 6.604746 \n",
      "Epoch 4 /200, Training Loss = 6.037163 , Validation Loss = 6.599680 \n",
      "Epoch 5 /200, Training Loss = 5.956763 , Validation Loss = 6.595480 \n",
      "Epoch 6 /200, Training Loss = 5.880852 , Validation Loss = 6.574459 \n",
      "Epoch 7 /200, Training Loss = 5.810875 , Validation Loss = 6.573510 \n",
      "Epoch 8 /200, Training Loss = 5.739761 , Validation Loss = 6.571457 \n",
      "Epoch 9 /200, Training Loss = 5.679635 , Validation Loss = 6.582091 \n",
      "Epoch 10 /200, Training Loss = 5.620070 , Validation Loss = 6.585753 \n",
      "Epoch 11 /200, Training Loss = 5.570145 , Validation Loss = 6.593592 \n",
      "Epoch 12 /200, Training Loss = 5.519672 , Validation Loss = 6.588217 \n",
      "Epoch 13 /200, Training Loss = 5.474669 , Validation Loss = 6.596014 \n",
      "Epoch 14 /200, Training Loss = 5.429639 , Validation Loss = 6.592329 \n",
      "Epoch 15 /200, Training Loss = 5.394632 , Validation Loss = 6.584241 \n",
      "Epoch 16 /200, Training Loss = 5.357909 , Validation Loss = 6.595021 \n",
      "Epoch 17 /200, Training Loss = 5.323279 , Validation Loss = 6.593795 \n",
      "Epoch 18 /200, Training Loss = 5.292428 , Validation Loss = 6.578591 \n",
      "Epoch 19 /200, Training Loss = 5.267911 , Validation Loss = 6.584061 \n",
      "Epoch 20 /200, Training Loss = 5.234225 , Validation Loss = 6.569647 \n",
      "Epoch 21 /200, Training Loss = 5.207934 , Validation Loss = 6.556638 \n",
      "Epoch 22 /200, Training Loss = 5.181971 , Validation Loss = 6.543000 \n",
      "Epoch 23 /200, Training Loss = 5.159019 , Validation Loss = 6.530795 \n",
      "Epoch 24 /200, Training Loss = 5.139188 , Validation Loss = 6.521231 \n",
      "Epoch 25 /200, Training Loss = 5.118350 , Validation Loss = 6.517635 \n",
      "Epoch 26 /200, Training Loss = 5.096592 , Validation Loss = 6.504309 \n",
      "Epoch 27 /200, Training Loss = 5.087849 , Validation Loss = 6.501111 \n",
      "Epoch 28 /200, Training Loss = 5.063604 , Validation Loss = 6.470443 \n",
      "Epoch 29 /200, Training Loss = 5.040524 , Validation Loss = 6.453477 \n",
      "Epoch 30 /200, Training Loss = 5.021133 , Validation Loss = 6.433408 \n",
      "Epoch 31 /200, Training Loss = 5.010231 , Validation Loss = 6.425098 \n",
      "Epoch 32 /200, Training Loss = 4.991883 , Validation Loss = 6.408669 \n",
      "Epoch 33 /200, Training Loss = 4.981716 , Validation Loss = 6.398742 \n",
      "Epoch 34 /200, Training Loss = 4.969553 , Validation Loss = 6.391373 \n",
      "Epoch 35 /200, Training Loss = 4.956832 , Validation Loss = 6.367332 \n",
      "Epoch 36 /200, Training Loss = 4.946887 , Validation Loss = 6.356713 \n",
      "Epoch 37 /200, Training Loss = 4.930384 , Validation Loss = 6.341636 \n",
      "Epoch 38 /200, Training Loss = 4.916566 , Validation Loss = 6.322679 \n",
      "Epoch 39 /200, Training Loss = 4.904959 , Validation Loss = 6.306260 \n",
      "Epoch 40 /200, Training Loss = 4.898708 , Validation Loss = 6.302380 \n",
      "Epoch 41 /200, Training Loss = 4.884766 , Validation Loss = 6.292655 \n",
      "Epoch 42 /200, Training Loss = 4.874192 , Validation Loss = 6.278633 \n",
      "Epoch 43 /200, Training Loss = 4.869702 , Validation Loss = 6.271883 \n",
      "Epoch 44 /200, Training Loss = 4.850926 , Validation Loss = 6.255081 \n",
      "Epoch 45 /200, Training Loss = 4.852473 , Validation Loss = 6.249629 \n",
      "Epoch 46 /200, Training Loss = 4.830841 , Validation Loss = 6.223999 \n",
      "Epoch 47 /200, Training Loss = 4.821885 , Validation Loss = 6.216663 \n",
      "Epoch 48 /200, Training Loss = 4.812188 , Validation Loss = 6.208337 \n",
      "Epoch 49 /200, Training Loss = 4.801623 , Validation Loss = 6.198552 \n",
      "Epoch 50 /200, Training Loss = 4.793996 , Validation Loss = 6.188916 \n",
      "Epoch 51 /200, Training Loss = 4.790239 , Validation Loss = 6.183316 \n",
      "Epoch 52 /200, Training Loss = 4.772971 , Validation Loss = 6.169998 \n",
      "Epoch 53 /200, Training Loss = 4.762926 , Validation Loss = 6.156807 \n",
      "Epoch 54 /200, Training Loss = 4.755779 , Validation Loss = 6.146466 \n",
      "Epoch 55 /200, Training Loss = 4.746627 , Validation Loss = 6.137995 \n",
      "Epoch 56 /200, Training Loss = 4.736039 , Validation Loss = 6.133693 \n",
      "Epoch 57 /200, Training Loss = 4.728525 , Validation Loss = 6.124622 \n",
      "Epoch 58 /200, Training Loss = 4.716533 , Validation Loss = 6.112434 \n",
      "Epoch 59 /200, Training Loss = 4.706371 , Validation Loss = 6.111820 \n",
      "Epoch 60 /200, Training Loss = 4.696817 , Validation Loss = 6.098476 \n",
      "Epoch 61 /200, Training Loss = 4.688988 , Validation Loss = 6.095859 \n",
      "Epoch 62 /200, Training Loss = 4.680332 , Validation Loss = 6.091573 \n",
      "Epoch 63 /200, Training Loss = 4.667524 , Validation Loss = 6.088266 \n",
      "Epoch 64 /200, Training Loss = 4.658823 , Validation Loss = 6.081425 \n",
      "Epoch 65 /200, Training Loss = 4.650881 , Validation Loss = 6.074431 \n",
      "Epoch 66 /200, Training Loss = 4.640075 , Validation Loss = 6.063363 \n",
      "Epoch 67 /200, Training Loss = 4.628979 , Validation Loss = 6.066054 \n",
      "Epoch 68 /200, Training Loss = 4.620183 , Validation Loss = 6.060541 \n",
      "Epoch 69 /200, Training Loss = 4.609596 , Validation Loss = 6.058296 \n",
      "Epoch 70 /200, Training Loss = 4.600058 , Validation Loss = 6.054880 \n",
      "Epoch 71 /200, Training Loss = 4.588569 , Validation Loss = 6.041988 \n",
      "Epoch 72 /200, Training Loss = 4.580106 , Validation Loss = 6.051434 \n",
      "Epoch 73 /200, Training Loss = 4.570425 , Validation Loss = 6.048962 \n",
      "Epoch 74 /200, Training Loss = 4.562632 , Validation Loss = 6.044729 \n",
      "Epoch 75 /200, Training Loss = 4.552154 , Validation Loss = 6.042722 \n",
      "Epoch 76 /200, Training Loss = 4.540215 , Validation Loss = 6.032038 \n",
      "Epoch 77 /200, Training Loss = 4.531333 , Validation Loss = 6.026350 \n",
      "Epoch 78 /200, Training Loss = 4.521224 , Validation Loss = 6.028251 \n",
      "Epoch 79 /200, Training Loss = 4.509133 , Validation Loss = 6.024897 \n",
      "Epoch 80 /200, Training Loss = 4.498747 , Validation Loss = 6.025211 \n",
      "Epoch 81 /200, Training Loss = 4.490479 , Validation Loss = 6.019733 \n",
      "Epoch 82 /200, Training Loss = 4.479897 , Validation Loss = 6.015097 \n",
      "Epoch 83 /200, Training Loss = 4.471307 , Validation Loss = 6.020735 \n",
      "Epoch 84 /200, Training Loss = 4.460161 , Validation Loss = 6.019680 \n",
      "Epoch 85 /200, Training Loss = 4.451109 , Validation Loss = 6.011167 \n",
      "Epoch 86 /200, Training Loss = 4.441162 , Validation Loss = 6.004306 \n",
      "Epoch 87 /200, Training Loss = 4.429933 , Validation Loss = 6.002717 \n",
      "Epoch 88 /200, Training Loss = 4.421198 , Validation Loss = 6.002961 \n",
      "Epoch 89 /200, Training Loss = 4.409304 , Validation Loss = 5.993493 \n",
      "Epoch 90 /200, Training Loss = 4.399867 , Validation Loss = 5.994472 \n",
      "Epoch 91 /200, Training Loss = 4.389467 , Validation Loss = 5.995176 \n",
      "Epoch 92 /200, Training Loss = 4.378505 , Validation Loss = 5.997237 \n",
      "Epoch 93 /200, Training Loss = 4.368351 , Validation Loss = 5.988073 \n",
      "Epoch 94 /200, Training Loss = 4.358371 , Validation Loss = 5.989040 \n",
      "Epoch 95 /200, Training Loss = 4.348733 , Validation Loss = 5.988884 \n",
      "Epoch 96 /200, Training Loss = 4.338530 , Validation Loss = 5.986406 \n",
      "Epoch 97 /200, Training Loss = 4.328894 , Validation Loss = 5.986619 \n",
      "Epoch 98 /200, Training Loss = 4.318112 , Validation Loss = 5.981817 \n",
      "Epoch 99 /200, Training Loss = 4.309613 , Validation Loss = 5.979177 \n",
      "Epoch 100 /200, Training Loss = 4.298639 , Validation Loss = 5.982059 \n",
      "Epoch 101 /200, Training Loss = 4.290329 , Validation Loss = 5.976401 \n",
      "Epoch 102 /200, Training Loss = 4.277583 , Validation Loss = 5.978244 \n",
      "Epoch 103 /200, Training Loss = 4.268868 , Validation Loss = 5.975850 \n",
      "Epoch 104 /200, Training Loss = 4.258070 , Validation Loss = 5.976560 \n",
      "Epoch 105 /200, Training Loss = 4.248359 , Validation Loss = 5.974037 \n",
      "Epoch 106 /200, Training Loss = 4.237941 , Validation Loss = 5.976648 \n",
      "Epoch 107 /200, Training Loss = 4.227092 , Validation Loss = 5.982861 \n",
      "Epoch 108 /200, Training Loss = 4.219799 , Validation Loss = 5.979700 \n",
      "Epoch 109 /200, Training Loss = 4.207537 , Validation Loss = 5.981370 \n",
      "Epoch 110 /200, Training Loss = 4.196908 , Validation Loss = 5.990980 \n",
      "Epoch 111 /200, Training Loss = 4.190641 , Validation Loss = 5.989241 \n",
      "Epoch 112 /200, Training Loss = 4.178175 , Validation Loss = 5.983563 \n",
      "Epoch 113 /200, Training Loss = 4.168093 , Validation Loss = 6.000746 \n",
      "Epoch 114 /200, Training Loss = 4.157231 , Validation Loss = 5.981523 \n",
      "Epoch 115 /200, Training Loss = 4.150497 , Validation Loss = 6.004209 \n",
      "Epoch 116 /200, Training Loss = 4.138522 , Validation Loss = 5.998662 \n",
      "Epoch 117 /200, Training Loss = 4.127814 , Validation Loss = 6.002860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 /200, Training Loss = 4.118588 , Validation Loss = 5.996413 \n",
      "Epoch 119 /200, Training Loss = 4.108665 , Validation Loss = 5.983416 \n",
      "Epoch 120 /200, Training Loss = 4.100736 , Validation Loss = 5.999384 \n",
      "Epoch 121 /200, Training Loss = 4.091658 , Validation Loss = 5.979251 \n",
      "Epoch 122 /200, Training Loss = 4.081597 , Validation Loss = 5.999629 \n",
      "Epoch 123 /200, Training Loss = 4.071713 , Validation Loss = 5.985387 \n",
      "Epoch 124 /200, Training Loss = 4.062766 , Validation Loss = 6.003527 \n",
      "Epoch 125 /200, Training Loss = 4.051444 , Validation Loss = 6.001935 \n",
      "Epoch 126 /200, Training Loss = 4.044618 , Validation Loss = 5.986883 \n",
      "Epoch 127 /200, Training Loss = 4.033751 , Validation Loss = 5.993674 \n",
      "Epoch 128 /200, Training Loss = 4.023991 , Validation Loss = 5.989610 \n",
      "Epoch 129 /200, Training Loss = 4.020575 , Validation Loss = 5.989608 \n",
      "Epoch 130 /200, Training Loss = 4.008860 , Validation Loss = 6.007221 \n",
      "Epoch 131 /200, Training Loss = 4.003504 , Validation Loss = 6.005090 \n",
      "Epoch 132 /200, Training Loss = 3.993489 , Validation Loss = 6.008564 \n",
      "Epoch 133 /200, Training Loss = 3.982756 , Validation Loss = 6.001118 \n",
      "Epoch 134 /200, Training Loss = 3.977115 , Validation Loss = 6.007002 \n",
      "Epoch 135 /200, Training Loss = 3.967116 , Validation Loss = 6.003234 \n",
      "Epoch 136 /200, Training Loss = 3.956794 , Validation Loss = 5.992321 \n",
      "Epoch 137 /200, Training Loss = 3.949265 , Validation Loss = 5.996418 \n",
      "Epoch 138 /200, Training Loss = 3.939781 , Validation Loss = 5.991150 \n",
      "Epoch 139 /200, Training Loss = 3.931389 , Validation Loss = 5.994713 \n",
      "Epoch 140 /200, Training Loss = 3.922910 , Validation Loss = 5.984375 \n",
      "Epoch 141 /200, Training Loss = 3.914257 , Validation Loss = 5.982808 \n",
      "Epoch 142 /200, Training Loss = 3.905528 , Validation Loss = 5.980204 \n",
      "Epoch 143 /200, Training Loss = 3.897217 , Validation Loss = 5.974701 \n",
      "Epoch 144 /200, Training Loss = 3.889125 , Validation Loss = 5.978086 \n",
      "Epoch 145 /200, Training Loss = 3.880466 , Validation Loss = 5.967750 \n",
      "Epoch 146 /200, Training Loss = 3.870921 , Validation Loss = 5.967224 \n",
      "Epoch 147 /200, Training Loss = 3.865170 , Validation Loss = 5.976769 \n",
      "Epoch 148 /200, Training Loss = 3.854838 , Validation Loss = 5.970696 \n",
      "Epoch 149 /200, Training Loss = 3.846760 , Validation Loss = 5.956366 \n",
      "Epoch 150 /200, Training Loss = 3.839338 , Validation Loss = 5.961705 \n",
      "Epoch 151 /200, Training Loss = 3.833309 , Validation Loss = 5.952679 \n",
      "Epoch 152 /200, Training Loss = 3.825194 , Validation Loss = 5.964202 \n",
      "Epoch 153 /200, Training Loss = 3.816667 , Validation Loss = 5.950075 \n",
      "Epoch 154 /200, Training Loss = 3.810687 , Validation Loss = 5.949979 \n",
      "Epoch 155 /200, Training Loss = 3.801662 , Validation Loss = 5.944249 \n",
      "Epoch 156 /200, Training Loss = 3.794499 , Validation Loss = 5.939878 \n",
      "Epoch 157 /200, Training Loss = 3.785192 , Validation Loss = 5.943636 \n",
      "Epoch 158 /200, Training Loss = 3.777536 , Validation Loss = 5.938653 \n",
      "Epoch 159 /200, Training Loss = 3.769137 , Validation Loss = 5.927140 \n",
      "Epoch 160 /200, Training Loss = 3.761703 , Validation Loss = 5.933818 \n",
      "Epoch 161 /200, Training Loss = 3.753808 , Validation Loss = 5.913333 \n",
      "Epoch 162 /200, Training Loss = 3.745685 , Validation Loss = 5.912253 \n",
      "Epoch 163 /200, Training Loss = 3.740045 , Validation Loss = 5.908681 \n",
      "Epoch 164 /200, Training Loss = 3.730627 , Validation Loss = 5.904103 \n",
      "Epoch 165 /200, Training Loss = 3.725436 , Validation Loss = 5.905821 \n",
      "Epoch 166 /200, Training Loss = 3.718993 , Validation Loss = 5.901640 \n",
      "Epoch 167 /200, Training Loss = 3.708996 , Validation Loss = 5.896821 \n",
      "Epoch 168 /200, Training Loss = 3.699472 , Validation Loss = 5.872831 \n",
      "Epoch 169 /200, Training Loss = 3.691586 , Validation Loss = 5.874340 \n",
      "Epoch 170 /200, Training Loss = 3.685891 , Validation Loss = 5.867792 \n",
      "Epoch 171 /200, Training Loss = 3.679345 , Validation Loss = 5.868525 \n",
      "Epoch 172 /200, Training Loss = 3.669515 , Validation Loss = 5.855145 \n",
      "Epoch 173 /200, Training Loss = 3.662248 , Validation Loss = 5.845448 \n",
      "Epoch 174 /200, Training Loss = 3.656209 , Validation Loss = 5.849164 \n",
      "Epoch 175 /200, Training Loss = 3.646954 , Validation Loss = 5.828683 \n",
      "Epoch 176 /200, Training Loss = 3.641960 , Validation Loss = 5.835624 \n",
      "Epoch 177 /200, Training Loss = 3.632276 , Validation Loss = 5.821841 \n",
      "Epoch 178 /200, Training Loss = 3.625547 , Validation Loss = 5.813104 \n",
      "Epoch 179 /200, Training Loss = 3.619092 , Validation Loss = 5.814035 \n",
      "Epoch 180 /200, Training Loss = 3.610466 , Validation Loss = 5.799699 \n",
      "Epoch 181 /200, Training Loss = 3.603868 , Validation Loss = 5.795909 \n",
      "Epoch 182 /200, Training Loss = 3.597217 , Validation Loss = 5.792477 \n",
      "Epoch 183 /200, Training Loss = 3.588707 , Validation Loss = 5.787064 \n",
      "Epoch 184 /200, Training Loss = 3.582011 , Validation Loss = 5.773712 \n",
      "Epoch 185 /200, Training Loss = 3.575054 , Validation Loss = 5.773156 \n",
      "Epoch 186 /200, Training Loss = 3.567917 , Validation Loss = 5.762496 \n",
      "Epoch 187 /200, Training Loss = 3.560374 , Validation Loss = 5.758338 \n",
      "Epoch 188 /200, Training Loss = 3.553290 , Validation Loss = 5.746250 \n",
      "Epoch 189 /200, Training Loss = 3.546632 , Validation Loss = 5.747470 \n",
      "Epoch 190 /200, Training Loss = 3.539699 , Validation Loss = 5.744402 \n",
      "Epoch 191 /200, Training Loss = 3.532237 , Validation Loss = 5.731852 \n",
      "Epoch 192 /200, Training Loss = 3.525347 , Validation Loss = 5.728942 \n",
      "Epoch 193 /200, Training Loss = 3.518426 , Validation Loss = 5.725609 \n",
      "Epoch 194 /200, Training Loss = 3.512142 , Validation Loss = 5.725184 \n",
      "Epoch 195 /200, Training Loss = 3.503862 , Validation Loss = 5.713894 \n",
      "Epoch 196 /200, Training Loss = 3.497100 , Validation Loss = 5.709027 \n",
      "Epoch 197 /200, Training Loss = 3.490774 , Validation Loss = 5.704565 \n",
      "Epoch 198 /200, Training Loss = 3.484343 , Validation Loss = 5.704838 \n",
      "Epoch 199 /200, Training Loss = 3.476399 , Validation Loss = 5.692635 \n",
      "Epoch 200 /200, Training Loss = 3.469203 , Validation Loss = 5.689109 \n"
     ]
    }
   ],
   "source": [
    "validationset = trainingset[-500:]\n",
    "validationlabels = traininglabels[-500:]\n",
    "trainingset = trainingset[:2500]\n",
    "trainingslabels = traininglabels[2500:]\n",
    "\n",
    "Batch_Size = 25\n",
    "Epochs_Losses = {}\n",
    "Validation_Losses = {}\n",
    "Epoch_Weights = {}\n",
    "Epoch_Bias = {}\n",
    "for i in range(200):                                         #Epochs\n",
    "    NN.WeightGradientIntialization()\n",
    "    NN.BiasGradientIntialization()\n",
    "    Reg = 0.1\n",
    "    Epoch_Loss = 0\n",
    "    Valid_Loss = 0\n",
    "    count = 0 \n",
    "    for j in range(trainingset.shape[0]):                         #Training\n",
    "        NN.ScoresGradientIntialization()\n",
    "        NN.SetInput(trainingset[j].reshape(-1 , 3072))\n",
    "        NNT = NeuralNetworkTrain(NN.NeuralNetwork, NN.weights, NN.Bias)\n",
    "        NNT.ForwardPass()\n",
    "        D_Weights , D_Bias = NNT.BackPropagation(traininglabels[j])\n",
    "        NN.DW = NNT.UpdateGradients(NN.DW ,D_Weights , Reg) \n",
    "        NN.DB += D_Bias[::-1]\n",
    "        count +=1\n",
    "        if count == Batch_Size:\n",
    "            NN.DW = AverageGradient(NN.DW , Batch_Size)\n",
    "            NN.DB = AverageGradient(NN.DB , Batch_Size)\n",
    "            NN.weights = NNT.UpdateWeights(1e-4, NN.DW)\n",
    "            NN.Bias = NNT.UpdateBias(1e-4, NN.DB)\n",
    "            count = 0\n",
    "            NN.WeightGradientIntialization()\n",
    "            NN.BiasGradientIntialization()\n",
    "    Epoch_Loss = EpochLoss(trainingset,traininglabels, NN , NNT) / trainingset.shape[0]              #Training Loss\n",
    "    Valid_Loss = EpochLoss(validationset,validationlabels, NN , NNT) / validationset.shape[0]              #Validation Loss\n",
    "    Epochs_Losses[i+1] = Epoch_Loss\n",
    "    Validation_Losses[i+1] = Valid_Loss\n",
    "    Epoch_Weights[i+1] = copy.deepcopy(NN.weights)\n",
    "    Epoch_Bias[i+1] = copy.deepcopy(NN.Bias)\n",
    "    print (\"Epoch %d /200, Training Loss = %f , Validation Loss = %f \" % (i+1,Epoch_Loss , Valid_Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Epochs_Losses.keys() , Epochs_Losses.values(), color = \"red\" , label = \"Training Loss\")\n",
    "plt.plot(Validation_Losses.keys() , Validation_Losses.values(), color = \"blue\" , label = \"Validation Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Correct Classification Count and Percentage: \n",
      "daisy: 49 , 0.365672\n",
      "dandelion: 77 , 0.574627\n",
      "roses: 56 , 0.417910\n",
      "sunflowers: 75 , 0.559701\n",
      "tulips: 58 , 0.432836\n"
     ]
    }
   ],
   "source": [
    "ClassesCorrectCount = {0:0 , 1:0 , 2:0 , 3:0 , 4:0}\n",
    "Classes = [\"daisy\",\"dandelion\", \"roses\",\"sunflowers\",\"tulips\"]\n",
    "for i in range(testset.shape[0]): \n",
    "    NN.SetInput(testset[i].reshape(-1,3072))\n",
    "    NNT = NeuralNetworkTrain(NN.NeuralNetwork, Epoch_Weights[198], Epoch_Bias[198])\n",
    "    NNT.ForwardPass()\n",
    "    Output = NN.NeuralNetwork[-1][0]\n",
    "    Prediction = np.argmax(Output)\n",
    "    if Prediction == testlabels[i]:\n",
    "        ClassesCorrectCount[testlabels[i]] +=1\n",
    "        \n",
    "ClassAccuarcies ={}\n",
    "for index in range(len(Classes)): \n",
    "    ClassAccuarcies[index] = ClassesCorrectCount[index] / 134\n",
    "\n",
    "print(\"Class Correct Classification Count and Percentage: \")\n",
    "for i in range(len(Classes)):\n",
    "    print(Classes[i] +\": %d , %f\" % (ClassesCorrectCount[i] , ClassAccuarcies[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Count: 315 / 670\n",
      "ACCR: 0.470149\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct Count: %d / %d\" % (sum(ClassesCorrectCount.values()) , testset.shape[0]))\n",
    "ACCR = sum(ClassesCorrectCount.values()) / testset.shape[0]\n",
    "print(\"ACCR: %f\" % ACCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
